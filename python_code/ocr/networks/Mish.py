'''
mish.py 
Usage: similar to torch.nn.ReLU()...and torch.autograd.Function 
# Author: Ty Nguyen
# Contact: tynguyen.tech@gmail.com
'''
import torch
import torch.nn as nn
import torch.nn.functional as F


class Mish_func(torch.autograd.Function):
    @staticmethod
    def forward(ctx, i):
        result = i * torch.tanh(F.softplus(i))
        ctx.save_for_backward(i)
        return result

    @staticmethod
    def backward(ctx, grad_output):
        i = ctx.saved_variables[0]

        v = 1. + i.exp()
        h = v.log()
        grad_gh = 1./h.cosh().pow_(2)

        # Note that grad_hv * grad_vx = sigmoid(x)
        #grad_hv = 1./v
        #grad_vx = i.exp()

        grad_hx = i.sigmoid()

        grad_gx = grad_gh * grad_hx  # grad_hv * grad_vx

        grad_f = torch.tanh(F.softplus(i)) + i * grad_gx

        return grad_output * grad_f


class Mish(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        pass

    def forward(self, input_tensor):
        return Mish_func.apply(input_tensor)
